# -*- coding: utf-8 -*-
"""heartdisese

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OcpMPwxtzOkALOsEj2eKaAu-zD2fxYY7
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report
from sklearn.ensemble import GradientBoostingClassifier
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/heart disese/heart.csv')

df

X = df.drop('target', axis=1)
y = df['target']
# Identifying numerical and categorical columns
numerical_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']
categorical_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']

# Define the preprocessing transformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),  # Normalization of numerical features
        ('cat', OneHotEncoder(), categorical_cols)  # One-hot encoding for categorical features
    ]
)

# Applying the preprocessing to the feature data
X_preprocessed = preprocessor.fit_transform(X)

# Convert the preprocessed data into a DataFrame for easier viewing
X_preprocessed_df = pd.DataFrame(X_preprocessed.toarray() if hasattr(X_preprocessed, 'toarray') else X_preprocessed)

X_preprocessed_df.head()

# Checking for missing values and summarizing each column
missing_values = df.isnull().sum()
data_summary = df.describe(include='all')

missing_values, data_summary

import numpy as np

# Function to identify outliers based on the IQR method for numerical columns
def find_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]
    return outliers[[column]]

# Find outliers for each numerical column
outliers_dict = {col: find_outliers(df, col) for col in numerical_cols}

outliers_dict

# Descriptive analysis for each column in the dataset
descriptive_stats = df.describe(include='all')

# Displaying statistics including count, mean, std, min, and quartiles for each column
descriptive_stats

from scipy.stats import ttest_ind, chi2_contingency

# Statistical Analysis
# 1. T-tests for numerical features based on target variable (heart disease presence/absence)
ttest_results = {}
for col in numerical_cols:
    group_0 = df[df['target'] == 0][col]
    group_1 = df[df['target'] == 1][col]
    t_stat, p_val = ttest_ind(group_0, group_1, nan_policy='omit')
    ttest_results[col] = {"t-statistic": t_stat, "p-value": p_val}

# 2. Chi-square test for categorical features to check association with the target variable
chi2_results = {}
for col in categorical_cols:
    contingency_table = pd.crosstab(df[col], df['target'])
    chi2_stat, p_val, _, _ = chi2_contingency(contingency_table)
    chi2_results[col] = {"chi2-statistic": chi2_stat, "p-value": p_val}

ttest_results, chi2_results

import matplotlib.pyplot as plt
# Convert t-test and chi-square p-values to DataFrames for plotting
ttest_pvalues = pd.DataFrame(ttest_results).T['p-value']
chi2_pvalues = pd.DataFrame(chi2_results).T['p-value']

# Plotting T-test p-values for numerical features
plt.figure(figsize=(10, 5))
plt.bar(ttest_pvalues.index, ttest_pvalues, color='coral')
plt.axhline(0.05, color='red', linestyle='--', label='Significance Level (0.05)')
plt.xlabel('Numerical Features')
plt.ylabel('P-value')
plt.title('T-test P-values for Numerical Features')
plt.legend()
plt.show()

# Plotting Chi-square test p-values for categorical features
plt.figure(figsize=(10, 5))
plt.bar(chi2_pvalues.index, chi2_pvalues, color='lightblue')
plt.axhline(0.05, color='red', linestyle='--', label='Significance Level (0.05)')
plt.xlabel('Categorical Features')
plt.ylabel('P-value')
plt.title('Chi-square Test P-values for Categorical Features')
plt.legend()
plt.show()

# Ensure preprocessing output is converted to DataFrame
import pandas as pd

# Convert preprocessed data to DataFrame if needed
X = pd.DataFrame(X_preprocessed.toarray() if hasattr(X_preprocessed, 'toarray') else X_preprocessed)

# Split the data into training, validation, and testing sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Using LazyPredict to evaluate multiple models on the training and validation set
from lazypredict.Supervised import LazyClassifier
clf = LazyClassifier(random_state=42)

# Ensure labels are also converted to a Pandas Series
y_train = pd.Series(y_train)
y_val = pd.Series(y_val)

# Fit LazyClassifier
models, predictions = clf.fit(X_train, X_val, y_train, y_val)

# Display the models ranked by accuracy
print(models)

import matplotlib.pyplot as plt

# Sort models by Validation Accuracy for clear visualization
models_sorted = models.sort_values(by="Accuracy", ascending=False)

# Plotting
plt.figure(figsize=(12, 8))
plt.barh(models_sorted.index, models_sorted['Accuracy'], color='skyblue')
plt.xlabel('Validation Accuracy')
plt.title('Model Performance Comparison')
plt.gca().invert_yaxis()  # Invert y-axis to have the best model at the top
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

from lightgbm import LGBMClassifier
from sklearn.metrics import classification_report, accuracy_score
import matplotlib.pyplot as plt
import numpy as np

# Train the LGBMClassifier
lgbm_model = LGBMClassifier(random_state=42)
lgbm_model.fit(X_train, y_train)

# Evaluate the model on training, validation, and testing sets
train_accuracy = accuracy_score(y_train, lgbm_model.predict(X_train))
val_accuracy = accuracy_score(y_val, lgbm_model.predict(X_val))
test_accuracy = accuracy_score(y_test, lgbm_model.predict(X_test))

# Collect accuracy scores for visualization
data = {
    "Dataset": ["Training", "Validation", "Testing"],
    "Accuracy": [train_accuracy, val_accuracy, test_accuracy]
}

# Visualization of performance
plt.figure(figsize=(8, 6))
plt.bar(data["Dataset"], data["Accuracy"], color=['skyblue', 'orange', 'green'], alpha=0.8)
plt.ylim(0, 1.1)
plt.title("LGBMClassifier Accuracy Across Datasets", fontsize=14)
plt.xlabel("Dataset", fontsize=12)
plt.ylabel("Accuracy", fontsize=12)
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.tight_layout()
plt.show()

# Evaluate models using LazyPredict
clf = LazyClassifier(random_state=42)
models, predictions = clf.fit(X_train, X_val, y_train, y_val)

# Sort models by Validation Accuracy for clear visualization
models_sorted = models.sort_values(by="Accuracy", ascending=False)

# Plotting
plt.figure(figsize=(12, 8))
plt.barh(models_sorted.index, models_sorted['Accuracy'], color='skyblue')
plt.xlabel('Validation Accuracy')
plt.title('Model Performance Comparison')
plt.gca().invert_yaxis()  # Invert y-axis to have the best model at the top
plt.show()

from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier
from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier
from sklearn.semi_supervised import LabelPropagation, LabelSpreading
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report

# Initialize models
models = {
    "LGBMClassifier": LGBMClassifier(random_state=42),
    "XGBClassifier": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
    "RandomForestClassifier": RandomForestClassifier(random_state=42),
    "ExtraTreeClassifier": ExtraTreeClassifier(random_state=42),
    "LabelPropagation": LabelPropagation(),
    "LabelSpreading": LabelSpreading(),
    "BaggingClassifier": BaggingClassifier(random_state=42),
    "DecisionTreeClassifier": DecisionTreeClassifier(random_state=42),
    "ExtraTreesClassifier": ExtraTreesClassifier(random_state=42)
}

# Train-test split (using previous split)
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)  # Train the model
    y_pred = model.predict(X_val)  # Predict on validation set
    accuracy = accuracy_score(y_val, y_pred)
    report = classification_report(y_val, y_pred, output_dict=True)
    results[name] = {
        "Accuracy": accuracy,
        "Classification Report": report
    }

# Display results
results

import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier
from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier
from sklearn.semi_supervised import LabelPropagation, LabelSpreading
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

# Define models
models = {
    "LGBMClassifier": LGBMClassifier(random_state=42),
    "XGBClassifier": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
    "RandomForestClassifier": RandomForestClassifier(random_state=42),
    "ExtraTreeClassifier": ExtraTreeClassifier(random_state=42),
    "LabelPropagation": LabelPropagation(),
    "LabelSpreading": LabelSpreading(),
    "BaggingClassifier": BaggingClassifier(random_state=42),
    "DecisionTreeClassifier": DecisionTreeClassifier(random_state=42),
    "ExtraTreesClassifier": ExtraTreesClassifier(random_state=42)
}

# Initialize lists to store accuracies for each model
train_accuracies = []
val_accuracies = []
test_accuracies = []
model_names = []

# Train, validate, and test each model
for name, model in models.items():
    model.fit(X_train, y_train)

    # Training accuracy
    train_acc = accuracy_score(y_train, model.predict(X_train))
    train_accuracies.append(train_acc)

    # Validation accuracy
    val_acc = accuracy_score(y_val, model.predict(X_val))
    val_accuracies.append(val_acc)

    # Testing accuracy
    test_acc = accuracy_score(y_test, model.predict(X_test))
    test_accuracies.append(test_acc)

    # Store model name
    model_names.append(name)

# Plotting accuracies
plt.figure(figsize=(12, 8))
plt.plot(model_names, train_accuracies, label="Training Accuracy", marker='o')
plt.plot(model_names, val_accuracies, label="Validation Accuracy", marker='o')
plt.plot(model_names, test_accuracies, label="Testing Accuracy", marker='o')
plt.xlabel("Model")
plt.ylabel("Accuracy")
plt.title("Training, Validation, and Testing Accuracy for Each Model")
plt.xticks(rotation=45)
plt.legend()
plt.grid()
plt.tight_layout()
plt.show()

import pandas as pd

# Create a DataFrame to store the accuracy results for easy comparison
comparison_df = pd.DataFrame({
    "Model": model_names,
    "Training Accuracy": train_accuracies,
    "Validation Accuracy": val_accuracies,
    "Testing Accuracy": test_accuracies
})

# Sort the DataFrame based on Testing Accuracy and Validation Accuracy
comparison_df = comparison_df.sort_values(by=["Testing Accuracy", "Validation Accuracy"], ascending=False)

# Display the comparison results
print(comparison_df)

# Identify the best model
best_model_name = comparison_df.iloc[0]["Model"]
best_model_test_acc = comparison_df.iloc[0]["Testing Accuracy"]
best_model_val_acc = comparison_df.iloc[0]["Validation Accuracy"]

print(f"\nBest Model: {best_model_name}")
print(f"Testing Accuracy: {best_model_test_acc}")
print(f"Validation Accuracy: {best_model_val_acc}")

import matplotlib.pyplot as plt

# LGBMClassifier accuracies
train_accuracy = 1.00  # Training accuracy
val_accuracy = 0.987   # Validation accuracy
test_accuracy = 0.9935 # Testing accuracy

# Accuracy values and labels
accuracies = [train_accuracy, val_accuracy, test_accuracy]
labels = ["Training Accuracy", "Validation Accuracy", "Testing Accuracy"]

# Plotting the accuracies
plt.figure(figsize=(8, 5))
plt.bar(labels, accuracies, color=['skyblue', 'salmon', 'lightgreen'])
plt.ylim(0.9, 1.1)  # Set y-axis range for better visualization
plt.xlabel("Data Split")
plt.ylabel("Accuracy")
plt.title("LGBMClassifier Accuracy on Training, Validation, and Testing Sets")
plt.show()

